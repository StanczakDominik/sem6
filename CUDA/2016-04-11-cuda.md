# CUDA dzisiaj
Wieści z konferencji
* Nowa karta P100, na procesorach PASCAL.
* Nowa architektura

Wcześniej mieliśmy Intel - PCIe - GPU, PCIe bandwidth 16GB/s
Intel chce mieć PCIe, więc NVidia robi tak:
> Niech intel idzie się bawić z PCI do swojej piaskownicy

POWER (z ubuntu) - nvlink - Pascal
> Nie wiem co to jest POWER, dowiedziałem się w zeszłym tygodniu że jest

Bez kopiowania danych między hostem a devicem!!!! Brak rozdziału na czas działania i kopiowania.

Trochę uderzają w Intela <- wycofuje się z serwerów obliczeniowych?


* Karty M40, M4
M40 - bardzo duże, bardzo drogie, deep learning, duża sieć z uczeniem. Uczenie wymaga dużej pamięci, używanie już nie - uczymy samochód jeździć w fabryce na M40, a potem inteligencję przegrywamy do małej M4

FPGA już niepotrzebne! Bardzo szybko, jedno zadanie
Sprzedają komplet dla automated car.
Samojeżdżące samochody
>Ford - wy macie samochody jeżdżące, ale my mamy jeżdżące po śniegu!

Ograniczona widoczność, brak pasów, ciemno, nie można polegać na kamerach

U nas na ssn: deep learning do niczego, nie działa
A TU ZONK! :D

Czas uczenia samochodu z kierowcą wewnątrz: 8h

Facebook pokazał projekt: deep learningiem nauczył sieć co jest na zdjęciach
> no, z fejsa. hyhy

"Narysuj mi krajobraz" - i tworzy sztuczne zdjęcie krajobrazu.
"Do tego krajobrazu dołóż zachód słońca" - potrafiła to zrobić
"A teraz zrób mi z tego impresjonistę


>flickr.co/photos/krzysztofzkabat

GDX-1, cena 130k dolarów - moc obliczeniowa 250 serwerów, tańszy bezpośrednio, mniej prądu... powerrr

Ktoś zmienia slajd -> cała sala ręce w górę i cyk fotkę

Śledzenie konkurencji, dyktafon w kieszeni i aparat

Bez szczegółów!

* Toyota - keynote - ciekawy wniosek:
> Ile potrzeba prądu żeby autonomiczny samochód mógł pojechać?

Wóz nabity elektroniką, kilka kW. Człowiek za kierownicą: 30W.
>Dlaczego komputery poszły w tą stronę? Ewolucja doprowadziła człowieka do takiego stanu, bo energia jest bardzo kosztowna - człowiek jest ewoucyjnie energooszczędny. Jednocześnie complexity człowieka jest ogromne. Komputery odwrotnie - low complexity, high power.

Ten deep learning: wiemy że działa, w sumie nie wiemy jak działa, zróbmy skomplikowaną strukturę i oszczędzajmy na prądzie (M40 -> M4)

* Co potrafi Maxwell
Fermi 2.x, Kepler 3.x, Maxwell 3.5, Pascal 5.

Od Keplera: można rekurencyjnie wywoływać kernele.

```c
__global__ void kernel(...)
{
    if (threadIdx.x ==0)
    {
        kernel<<<...>>>(...)
        cudaDeviceSynchronize(); //trzeba uważać czy czasem nie blokuje całego bloku
    }
}
```

nasz wątek może zdecydować o uruchomieniu nowego kernela. Nie ma ograniczenia że tylko CPU może je odpalać! Koszt: synchronizacja, kopiowanie GPU -> CPU, jakiś if, nowy kernel... Synch i copy eliminowane
Recurrence limit jest domyślnie 32 ale można w kompilatorze zmniejszyć
### CUDA - volatile

```c
__global void myKernel(int* result)
{
   int tid = threadIdx.x;
   int ref1 = myArray[tid] * 1;    //pierwszy odczyt
   myArray[tid + 1] = 2;
   int ref2 = myArray[tid] * 1;    //drugi odczyt, kompilator widzi jako niepotrzebny: "references reuse"
   result[tid] = ref1 * ref2;     //błędny wynik
}
```
Ale w naszym przypadku mamy mieszanie między wątkami a wartościami w tablicy
> Imagine the warp!

Trzeba opatrzeć `myArray` identyfikatorkiem volatile

### CUDA - optymalizacja
1. Optymalizacja 1: jak najwięcej wątków i bloków
2. Opt. 2: Dużo obliczeń, mało kopiowania pamięci

```c
int* a;
a[tid] = a[tid] + 1; //jakośtam modyfikuje indeks w tablicy
```

Jeden odczyt, jeden zapis - dużo IO, mało obliczeń. Zamiast tego: odczytywać po cztery inty:
```c
int4 *b = (int4*)a;
      b[tid].x + 1
             y
             z
             w
```
> Chociaż nie! Może tak zadziała! Może być że tak będzie dobrze.

Lub: wątki przemieszczają się w pętli po tablicy.
```
xxxxxxxxxxxxxxxxxxxx
11111110000000000000
00000001111111000000
00000000000000111111
```
* Przesunięcia bitowe zamiast dzielenia, mnożenia przez 2
* Modulo też jest kosztowne

Optymalizujemy tam, gdzie to potrzebne.

### CUDA - pomiar wydajności

Profiler jest naprawdę zajebisty

API do timerów CUDA timers

```c
cudaEvent_t start, stop;
float time;
cudaEventCreate(&start);
cudaEventCreate(&stop);

cudaEventRecord(start, 0);
kernel<<<grid, threads>>>(...);
cudaEventRecord(stop, 0); //numer strumienia
cudaEventSynchronize(stop);
cudaEventElapsedTime(&time, start, stop); //czas w milisekundach, float

cudaEventDestroy(start);
cudaEventDestroy(stop);
```
API systemowe ma wadę że zależy od OS. Poza tym niedokładne.

Możemy tworzyć równoległe strumienie wykonania kodu

Biblioteka cutil: nieoficjalna. Limit: 1 karta graficzna.

### CUDA - optymalizacja
* Możliwości sprzętu $~$ prędkość dostępu do pamięci
* `__sin` - mniej dokładny, ale dużo szybszy!
* GTX pracują na floatach, raczej ich używać niż doubli. `1.02f; sinf; powf;`
* Pinned memory - nie jest zaalokowana gdziekolwiek w GPU czy w RAMie, tylko na początku stron - kopiowanie automatyczne. Troszeczkę lepiej. Ale trzeba wtedy użyć cudaMallocHost
* Dostęp do pamięci hosta z wnętrza kernela:
* Wyrównanie struktur w pamięci
```c
struct __align__(16){
    float a;
    float b;
    float c; //12 bajtów
}
```
16 bajtów jest i tak otrzymywane w jednym "strzale"
* Coalesced global memory access: dla wątków w warpie\half warpie
Dla 3M floatów, dla wyrównanego 356
* Jedną wielką tablicę podzielić na kilka malloców?
* Struktura może nam zepsuć odczyt. Róbmy structure of arrays zamiast array of structures
* Slajd 85/105
